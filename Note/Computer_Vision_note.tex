\documentclass{report}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage[a4paper, total={6in, 10in}]{geometry}

\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{tikz}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}

\renewcommand{\Re}{\mathbb{R}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\Large{Computer Vision Note}}
\author{Sagar Ojha}


\begin{document}
\maketitle
\pagebreak

%--------------------------------------------------------------------------------------------
%--------------------------------------------------------------------------------------------
\tableofcontents
\pagebreak
%--------------------------------------------------------------------------------------------
\chapter{Basic Computer Vision}

\section{Learning Resources}
\hspace{\parindent}\href{https://www.youtube.com/@firstprinciplesofcomputerv3258/playlists}{First Principles of Computer Vision} and \href{https://www.udacity.com/course/introduction-to-computer-vision--ud810?autoenroll=true}{Introduction to Computer Vision} are good places to start. The former one introduces mathematics that governs a certain idea and then develops the algorithm and its implementation in discrete space with adequate proofs and derivations while the latter one provides brief mathematical idea about the algorithms and provides some exercise and quizzes to test the knowledge. I would recommend starting with the latter one and watching some videos from the former one to get more idea about the proof of the algorithm.
%--------------------------------------------------------------------------------------------


%--------------------------------------------------------------------------------------------
\section{Image Processing}
\hspace{\parindent}We will use \href{https://en.wikipedia.org/wiki/Grayscale}{grayscaled image} for our works. The idea can be expanded to RBG images as well. Image (or image intensity) is basically a function of (x, y). Grayscale image can be obtained by some transformation (or function) applied on the pixel.

\subsection{Filters}
\hspace{\parindent}Filters are used to modify an image and extract features from image. We will be using \href{https://www.youtube.com/watch?v=ctn4MKATJOs&t=3s}{convolution filters}. Convolution is a \href{https://www.youtube.com/watch?v=Ma0YONjMZLI}{cross-correlation} but with the function first reflected about y-axis. Convolution is linear and shift invariant operation (or system). Often, we won't know what we the input function is being convolved with. So, if we want to know the function that the input convolutes with, then we can pass in unit impulse function to convolute with the unknown function. The resulting function will be the unknown function. Thhrefore, we get to know the previously unknown convolution function.

The convolution operation over 2D-discrete functions can be implemented using matrices called convolution masks/kernel/filter represented by $h$ in \ref{eqn:2DDiscreteConvolution}.

\begin{equation}
g[i,j]=\sum_{m=1} ^{M}\sum_{n=1} ^{N} f[m,n]h[i-m, j-n]
\label{eqn:2DDiscreteConvolution}
\end{equation}

Image functions/matrices are the inputs and we convolute them with the kernel matrices. Box, fuzzy, gaussian (which is also fuzzy but is standard or formalized) filters are applied using convolution. Gaussian filters can be separated such that the convolution time complexity can be minimized.

In general, this is the roadmap that led to matrix filters: we were motivated to modify images which led to the start of developing convolution filters. We started learning the 1D continuous convolution filter and then developed 2D discrete convolution filter. In 2D, the discrete convolution filter is implemented using a matrix. Also, in 1D discrete system, the filter would be realized using an array.
%--------------------------------------------------------------------------------------------

%--------------------------------------------------------------------------------------------
\subsection{Edge Detection}
\hspace{\parindent}We'll develop a Canny Edge detector which is a very popular algorithm to find the edges in an image. The first thing is to blur the image (yep the grayscale image) to reduce noise in the image. This is also a type of ``modification'' of image which is also done using convolution. The amount of blur will definitely impact the edge detection and blurring can be modified to our need. We'll use Gaussian blur as we assumed the noise is random and in such case Gaussian blur will perform the best. \textbf{gaussian\_filter($\sigma$)} function will take in the standard deviation as the input and return the Gaussian kernel. Size of the kernel $\approx 2\pi\sigma$. \textbf{convolution(f, h)} will take matrix \textbf{f} and convolute with another input matrix \textbf{h} and return the resulting matrix. An alternative to Gaussian filter could be a fuzzy filter.

The next step is to calculate the image gradient magnitude and direction. Sobel kernel could be used to get the first derivative of the image in horizontal and vertical directions. Sobel operation is also an edge detection method in itself. The kernel is convolved with the image which was obtained after filtering.

\begin{equation}
\nabla_x = \frac{1}{8}
\begin{bmatrix}
-1 & 0 & 1\\
-2 & 0 & 2\\
-1 & 0 & 1
\end{bmatrix},\;\;
\nabla_y = \frac{1}{8}
\begin{bmatrix}
-1 & -2  & -1\\
 0 &  0  &  0\\
 1 &  2  &  1\\
\end{bmatrix}
\end{equation}

\begin{equation}
\nabla_{mag} = \sqrt{\nabla_x^{2} + \nabla_y^{2}},\;\;
\nabla_{dir} = \arctan2(\nabla_y, \nabla_x)
\end{equation}

Gradient images don't have sharp or thin edges. So the next step is to find the local maximum pixel value in the direction of the gradient. A full scan of the image has to be performed. The local maximum will be kept as a possible edge and other pixels are zeroed. This is called non-maximum suppression method. 1D laplacian operator in the gradient direction could be implemented as well to get the local maximum from a sharp zero crossing. We will use a naive method of comparing the neighboring pixels rather.

After the non-maximum suppression, hysteresis thresholding is applied. $2$ threshold values are chosen. If a pixel is above the higher one, the pixel is marked as a strong edge, and if it is below the lower value, the pixel is marked as not an edge. If the pixel is in between the threshold values, then it gets marked as a weak edge. Finally, if the weak edge is connected to a strong edge, then the weak edge is marked to be a strong edge as well; marking the weak edge to a strong edge will help connect the weak edges which were at the neighborhood of the newly marked strong edge. After a full scan, the strong edges are considered as definite edge. This is what the algorithm outputs as the edge in an image.
%--------------------------------------------------------------------------------------------

%--------------------------------------------------------------------------------------------
\section{Hough Transform}
\subsection{Detecting Lines}
\hspace{\parindent}Lines represented as 
\begin{align}
	y = mx + c
	\label{eqn:StraightLine}
\end{align}
can be parametrized as
\begin{align}
	x \cos{\theta} + y \sin{\theta} = \rho
	\label{eqn:SinusoidParameterizedLine}
\end{align}
where $\rho$ is the perpendicular distance from the origin to the line and $\theta$ is the angle that the perpendicular line to Eq.~\ref{eqn:StraightLine} makes with the horizontal axis. In the \textit{Hough} space, a point $(x,y)$ is represented as a sinusoid given by Eq.~\ref{eqn:SinusoidParameterizedLine}.

In order to derive the parameterized representation in Eq.~\ref{eqn:SinusoidParameterizedLine}, consider a unit vector $[ \cos{\theta} \; \sin{\theta} ]^\text{T}$ pointed towards the point $(x, y)$ and the vector $[x \; y]^{\text{T}}$ whose tail is the origin and the head is at $(x,y)$ with the magnitude of $\rho$. Using the dot product, one gets
\begin{align}
	\begin{bmatrix}
		\cos{\theta} \\
		\sin{\theta}
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
		x \\
		y
	\end{bmatrix}
	& = \rho, \\
	x \cos{\theta} + y \sin{\theta} & = \rho.
	\label{eqn:SinusoidParameterizedLineDerivation}
\end{align}

% Hough transform plot
%\begin{figure}[h]
%\centering
%\begin{tikzpicture}
%	% Axes Left
%	\draw [->, very thick] (-5cm, 0cm) -- (-5cm, 4cm);
%	\draw [->, very thick] (-5cm, 0cm) -- (-1cm, 0cm);
%	
%	% Line
%	\draw [-, very thick, blue] (-5cm, 3cm) -- (-2cm, 0cm);
%	\node at () [] {$$};
%	
%	% Axes Right
%	\node at (0cm,0cm) [circle, draw=red!50, fill=red!5, very thick, minimum size=1.5cm]{$a = g(\vec w \cdot \vec x + b)$};
%	\draw [->, very thick] (1.4cm, 0cm) -- (5cm, 0cm);
%	\node at (5.2cm, 0cm)[]{$a$};
%\end{tikzpicture}
%\caption{Parameteric representation of point and line.}
%\label{fig:Houghtransform}
%\end{figure}

Considering the origin of the image at the lower-bottom part, the the image can be swept across with $0^{\circ} \leq \theta \leq 90^{\circ}$ and $0 \leq \rho \leq \ell$ where $\ell$ represents the diagonal length of the image; diagonal length of the image can be considered as the number of pixels along the image diagonal fo the implementation. Create an accumulator bin/matrix of appropriate size depending on the resolution as well as range of both $\theta$ and $\rho$. Then, collect the votes from each \textbf{edge points}. The cells with the largest votes represents the parameters of the line that the edges correspond to. One may draw the line using the parameters. I think the overall implementation would be extremely expensive.



\subsection{Detecting Circles}



%--------------------------------------------------------------------------------------------


\end{document}